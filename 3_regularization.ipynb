{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in _notmist.ipynb_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training set', (200000, 28, 28), (200000,))\n",
      "('Validation set', (10000, 28, 28), (10000,))\n",
      "('Test set', (10000, 28, 28), (10000,))\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    valid_dataset = save['valid_dataset']\n",
    "    valid_labels = save['valid_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "    del save  # hint to help gc free up memory\n",
    "    print('Training set', train_dataset.shape, train_labels.shape)\n",
    "    print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "    print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training set', (200000, 784), (200000, 10))\n",
      "('Validation set', (10000, 784), (10000, 10))\n",
      "('Test set', (10000, 784), (10000, 10))\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 2 to [0.0, 1.0, 0.0 ...], 3 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compue the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta_regul = tf.placeholder(tf.float32)\n",
    "  \n",
    "    # Variables.\n",
    "    weights = tf.Variable(\n",
    "      tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "    # Training computation.\n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels) + beta_regul * tf.nn.l2_loss(weights))\n",
    "  \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******** beta = 0.000100\n",
      "Initialized\n",
      "L2 regularization(beta=0.00010) Test accuracy: 86.5%\n",
      "******** beta = 0.000167\n",
      "Initialized\n",
      "L2 regularization(beta=0.00017) Test accuracy: 86.5%\n",
      "******** beta = 0.000278\n",
      "Initialized\n",
      "L2 regularization(beta=0.00028) Test accuracy: 87.0%\n",
      "******** beta = 0.000464\n",
      "Initialized\n",
      "L2 regularization(beta=0.00046) Test accuracy: 87.1%\n",
      "******** beta = 0.000774\n",
      "Initialized\n",
      "L2 regularization(beta=0.00077) Test accuracy: 87.9%\n",
      "******** beta = 0.001292\n",
      "Initialized\n",
      "L2 regularization(beta=0.00129) Test accuracy: 88.2%\n",
      "******** beta = 0.002154\n",
      "Initialized\n",
      "L2 regularization(beta=0.00215) Test accuracy: 88.2%\n",
      "******** beta = 0.003594\n",
      "Initialized\n",
      "L2 regularization(beta=0.00359) Test accuracy: 87.9%\n",
      "******** beta = 0.005995\n",
      "Initialized\n",
      "L2 regularization(beta=0.00599) Test accuracy: 87.8%\n",
      "******** beta = 0.010000\n",
      "Initialized\n",
      "L2 regularization(beta=0.01000) Test accuracy: 87.4%\n",
      "Best beta=0.001292, accuracy=88.2%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEOCAYAAACEiBAqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEueJwDnnSpwnAuecK3GeCJxzrsT9f0xf09sj5i/MAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f197c4cd310>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "betas = np.logspace(-4, -2, 10)\n",
    "accuracys = []\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "for beta in betas:\n",
    "    print('******** beta = %f'% beta)\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.initialize_all_variables().run()\n",
    "        print(\"Initialized\")\n",
    "        for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            # Generate a minibatch.\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            # and the value is the numpy array to feed to it.\n",
    "            _, l, predictions = session.run( [optimizer, loss, train_prediction], feed_dict=\n",
    "                                            {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul: beta})\n",
    "        print(\"L2 regularization(beta=%.5f) Test accuracy: %.1f%%\" % (\n",
    "            beta, accuracy(test_prediction.eval(), test_labels)))\n",
    "        accuracys.append(accuracy(test_prediction.eval(), test_labels))\n",
    " \n",
    "print('Best beta=%f, accuracy=%.1f%%' % (betas[np.argmax(accuracys)], max(accuracys)))\n",
    "plt.semilogx(betas, accuracys)\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization (logistic)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "hidden_size = 1024\n",
    " \n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    tf_beta = tf.placeholder(tf.float32)\n",
    " \n",
    "    # Variables.\n",
    "    W1 = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_size]))\n",
    "    b1 = tf.Variable(tf.zeros([hidden_size]))\n",
    " \n",
    "    W2 = tf.Variable(tf.truncated_normal([hidden_size, num_labels]))\n",
    "    b2 = tf.Variable(tf.zeros([num_labels]))\n",
    " \n",
    "    # Training computation.\n",
    "    y1 = tf.nn.relu(tf.matmul(tf_train_dataset, W1) + b1)\n",
    "    logits = tf.matmul(y1, W2) + b2\n",
    " \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    " \n",
    "    loss = loss + tf_beta * (tf.nn.l2_loss(W1) + tf.nn.l2_loss(b1) + tf.nn.l2_loss(W2) + tf.nn.l2_loss(b2))\n",
    " \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    " \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    " \n",
    "    y1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, W1) + b1)\n",
    "    valid_logits = tf.matmul(y1_valid, W2) + b2\n",
    "    valid_prediction = tf.nn.softmax(valid_logits)\n",
    " \n",
    "    y1_test = tf.nn.relu(tf.matmul(tf_test_dataset, W1) + b1)\n",
    "    test_logits = tf.matmul(y1_test, W2) + b2\n",
    "    test_prediction = tf.nn.softmax(test_logits)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******** beta = 0.000100\n",
      "Initialized\n",
      "L2 regularization(beta=0.00010) Test accuracy: 87.0%\n",
      "******** beta = 0.000167\n",
      "Initialized\n",
      "L2 regularization(beta=0.00017) Test accuracy: 87.5%\n",
      "******** beta = 0.000278\n",
      "Initialized\n",
      "L2 regularization(beta=0.00028) Test accuracy: 87.7%\n",
      "******** beta = 0.000464\n",
      "Initialized\n",
      "L2 regularization(beta=0.00046) Test accuracy: 88.6%\n",
      "******** beta = 0.000774\n",
      "Initialized\n",
      "L2 regularization(beta=0.00077) Test accuracy: 90.4%\n",
      "******** beta = 0.001292\n",
      "Initialized\n",
      "L2 regularization(beta=0.00129) Test accuracy: 92.1%\n",
      "******** beta = 0.002154\n",
      "Initialized\n",
      "L2 regularization(beta=0.00215) Test accuracy: 91.7%\n",
      "******** beta = 0.003594\n",
      "Initialized\n",
      "L2 regularization(beta=0.00359) Test accuracy: 90.8%\n",
      "******** beta = 0.005995\n",
      "Initialized\n",
      "L2 regularization(beta=0.00599) Test accuracy: 89.6%\n",
      "******** beta = 0.010000\n",
      "Initialized\n",
      "L2 regularization(beta=0.01000) Test accuracy: 88.6%\n",
      "Best beta=0.001292, accuracy=92.1%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEOCAYAAACD5gx6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIhJRSvAiIhGlBC8iElH/Dx/g0mG2wRJkAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f197c4cd310>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracys= []\n",
    "for beta in betas:\n",
    "    print('******** beta = %f'% beta)\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.initialize_all_variables().run()\n",
    "        print(\"Initialized\")\n",
    "        for step in range(num_steps):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            # Note: we could use better randomization across epochs.\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            # Generate a minibatch.\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            # and the value is the numpy array to feed to it.\n",
    "            feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels, tf_beta: beta}\n",
    "            _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        print(\"L2 regularization(beta=%.5f) Test accuracy: %.1f%%\" % ( beta, accuracy(test_prediction.eval(), test_labels)))\n",
    "        accuracys.append(accuracy(test_prediction.eval(), test_labels))\n",
    " \n",
    "print('Best beta=%f, accuracy=%.1f%%' % (betas[np.argmax(accuracys)], max(accuracys)))\n",
    "plt.semilogx(betas, accuracys)\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization (1-layer net)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 927.381653\n",
      "Minibatch accuracy: 9.4%\n",
      "Validation accuracy: 44.0%\n",
      "Minibatch loss at step 500: 229.863480\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 79.1%\n",
      "Minibatch loss at step 1000: 114.225166\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 80.2%\n",
      "Minibatch loss at step 1500: 53.648960\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 83.8%\n",
      "Minibatch loss at step 2000: 25.517511\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 86.1%\n",
      "Minibatch loss at step 2500: 12.836279\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 3000: 6.684408\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 87.1%\n",
      "Dropout Test accuracy: 92.3%\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "hidden_size = 1024\n",
    " \n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    tf_beta = tf.placeholder(tf.float32)\n",
    " \n",
    "    # Variables.\n",
    "    W1 = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_size]))\n",
    "    b1 = tf.Variable(tf.zeros([hidden_size]))\n",
    " \n",
    "    W2 = tf.Variable(tf.truncated_normal([hidden_size, num_labels]))\n",
    "    b2 = tf.Variable(tf.zeros([num_labels]))\n",
    " \n",
    "    # Training computation.\n",
    "    y1 = tf.nn.relu(tf.matmul(tf_train_dataset, W1) + b1)\n",
    "    y1 = tf.nn.dropout(y1, 0.5)  # Dropout\n",
    "    logits = tf.matmul(y1, W2) + b2\n",
    " \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    " \n",
    "    loss = loss + tf_beta * (tf.nn.l2_loss(W1) + tf.nn.l2_loss(b1) + tf.nn.l2_loss(W2) + tf.nn.l2_loss(b2))\n",
    " \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    " \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    " \n",
    "    y1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, W1) + b1)\n",
    "    valid_logits = tf.matmul(y1_valid, W2) + b2\n",
    "    valid_prediction = tf.nn.softmax(valid_logits)\n",
    " \n",
    "    y1_test = tf.nn.relu(tf.matmul(tf_test_dataset, W1) + b1)\n",
    "    test_logits = tf.matmul(y1_test, W2) + b2\n",
    "    test_prediction = tf.nn.softmax(test_logits)\n",
    " \n",
    "# Let's run it:\n",
    "num_steps = 3001\n",
    " \n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels, tf_beta: 0.001438}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "                valid_prediction.eval(), valid_labels))\n",
    "    print(\"Dropout Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 9.546898\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 15.2%\n",
      "Minibatch loss at step 500: 3.864041\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 1000: 2.310104\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 87.6%\n",
      "Minibatch loss at step 1500: 1.494854\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 2000: 1.134554\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 2500: 0.942665\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 3000: 0.794093\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 3500: 0.842916\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 90.6%\n",
      "Minibatch loss at step 4000: 0.548736\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 4500: 0.660358\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 5000: 0.527748\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 5500: 0.516571\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.0%\n",
      "Minibatch loss at step 6000: 0.490870\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.3%\n",
      "Minibatch loss at step 6500: 0.631114\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.3%\n",
      "Minibatch loss at step 7000: 0.452444\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 91.5%\n",
      "Minibatch loss at step 7500: 0.561959\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.6%\n",
      "Minibatch loss at step 8000: 0.463939\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.6%\n",
      "Minibatch loss at step 8500: 0.464790\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.6%\n",
      "Minibatch loss at step 9000: 0.392738\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 9500: 0.369693\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 10000: 0.444408\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 10500: 0.452954\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.8%\n",
      "Minibatch loss at step 11000: 0.481884\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 11500: 0.435193\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 12000: 0.402948\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.9%\n",
      "Final Test accuracy: 95.9%\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "fc1_size = 4096\n",
    "fc2_size = 2048\n",
    "fc3_size = 128\n",
    " \n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                      shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    tf_beta = tf.placeholder(tf.float32)\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    " \n",
    "    # Variables.\n",
    "    # stddev is very important!!!\n",
    "    W1 = tf.Variable(\n",
    "        tf.truncated_normal([image_size * image_size, fc1_size], stddev=np.sqrt(2.0 / (image_size * image_size))))\n",
    "    b1 = tf.Variable(tf.zeros([fc1_size]))\n",
    " \n",
    "    W2 = tf.Variable(tf.truncated_normal([fc1_size, fc2_size], stddev=np.sqrt(2.0 / fc1_size)))\n",
    "    b2 = tf.Variable(tf.zeros([fc2_size]))\n",
    " \n",
    "    W3 = tf.Variable(tf.truncated_normal([fc2_size, fc3_size], stddev=np.sqrt(2.0 / fc2_size)))\n",
    "    b3 = tf.Variable(tf.zeros([fc3_size]))\n",
    " \n",
    "    W4 = tf.Variable(tf.truncated_normal([fc3_size, num_labels], stddev=np.sqrt(2.0 / fc3_size)))\n",
    "    b4 = tf.Variable(tf.zeros([num_labels]))\n",
    " \n",
    "    # Training computation.\n",
    "    y1 = tf.nn.relu(tf.matmul(tf_train_dataset, W1) + b1)\n",
    "    # y1 = tf.nn.dropout(y1, 0.5)\n",
    " \n",
    "    y2 = tf.nn.relu(tf.matmul(y1, W2) + b2)\n",
    "    # y2 = tf.nn.dropout(y2, 0.5)\n",
    " \n",
    "    y3 = tf.nn.relu(tf.matmul(y2, W3) + b3)\n",
    "    # y3 = tf.nn.dropout(y3, 0.5)\n",
    " \n",
    "    logits = tf.matmul(y3, W4) + b4\n",
    " \n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    " \n",
    "    loss = loss + tf_beta * (tf.nn.l2_loss(W1) + tf.nn.l2_loss(b1) + tf.nn.l2_loss(W2) + tf.nn.l2_loss(b2) +\n",
    "                             tf.nn.l2_loss(W3) + tf.nn.l2_loss(b3) + tf.nn.l2_loss(W4) + tf.nn.l2_loss(b4))\n",
    " \n",
    "    # Optimizer\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, 1000, 0.7, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    " \n",
    "    y1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, W1) + b1)\n",
    "    y2_valid = tf.nn.relu(tf.matmul(y1_valid, W2) + b2)\n",
    "    y3_valid = tf.nn.relu(tf.matmul(y2_valid, W3) + b3)\n",
    "    valid_logits = tf.matmul(y3_valid, W4) + b4\n",
    "    valid_prediction = tf.nn.softmax(valid_logits)\n",
    " \n",
    "    y1_test = tf.nn.relu(tf.matmul(tf_test_dataset, W1) + b1)\n",
    "    y2_test = tf.nn.relu(tf.matmul(y1_test, W2) + b2)\n",
    "    y3_test = tf.nn.relu(tf.matmul(y2_test, W3) + b3)\n",
    "    test_logits = tf.matmul(y3_test, W4) + b4\n",
    "    test_prediction = tf.nn.softmax(test_logits)\n",
    " \n",
    "# Let's run it:\n",
    "num_steps = 12001\n",
    " \n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels, tf_beta: 0.001438}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "                valid_prediction.eval(), valid_labels))\n",
    "    print(\"Final Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
